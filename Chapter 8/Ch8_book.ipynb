{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chaper 8 - Intrinsic Curiosity Module\n",
    "#### Deep Reinforcement Learning *in Action*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 8.1 - setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from nes_py.wrappers import JoypadSpace #1: making the action space smaller by combining actions together\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT #action spaces (5 (simple), 12(complex) actions)\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, COMPLEX_MOVEMENT) #3: wraps the env's action space to 12 actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['NOOP'],\n",
       "  ['right'],\n",
       "  ['right', 'A'],\n",
       "  ['right', 'B'],\n",
       "  ['right', 'A', 'B'],\n",
       "  ['A'],\n",
       "  ['left']],\n",
       " [['NOOP'],\n",
       "  ['right'],\n",
       "  ['right', 'A'],\n",
       "  ['right', 'B'],\n",
       "  ['right', 'A', 'B'],\n",
       "  ['A'],\n",
       "  ['left'],\n",
       "  ['left', 'A'],\n",
       "  ['left', 'B'],\n",
       "  ['left', 'A', 'B'],\n",
       "  ['down'],\n",
       "  ['up']])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printed list of action sets\n",
    "SIMPLE_MOVEMENT, COMPLEX_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = True\n",
    "for step in range(500): #4: testing environment by taking random actions\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 8.2 - downsample state and convert to grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize #1: image-resizing function\n",
    "import numpy as np\n",
    "\n",
    "def downscale_obs(obs, new_size=(42,42), to_gray=True):\n",
    "    if to_gray:\n",
    "        return resize(obs, new_size, anti_aliasing=True).max(axis=2) #2: take the maximum values across the channel dimension -> convert into grayscale\n",
    "    else:\n",
    "        return resize(obs, new_size, anti_aliasing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13531e358>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.imshow(env.render(\"rgb_array\"))  #: full rgb original\n",
    "plt.imshow(downscale_obs(env.render(\"rgb_array\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 8.3 - preparing the states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque  #: fist-in-first-out data structure; with maxlen\n",
    "\n",
    "def prepare_state(state): #1: downscales state and converts to grayscale -> pytorch tensor -> adds a batch dim\n",
    "    return torch.from_numpy(downscale_obs(state, to_gray=True)).float().unsqueeze(dim=0)\n",
    "\n",
    "\n",
    "def prepare_multi_state(state1, state2): #2: adds the latest frame to the queue\n",
    "    state1 = state1.clone()\n",
    "    tmp = torch.from_numpy(downscale_obs(state2, to_gray=True)).float()\n",
    "    state1[0][0] = state1[0][1]\n",
    "    state1[0][1] = state1[0][2]\n",
    "    state1[0][2] = tmp\n",
    "    return state1\n",
    "\n",
    "\n",
    "def prepare_initial_state(state,N=3): #3: initial state: copies of the same frame -> adds a batch dim\n",
    "    state_ = torch.from_numpy(downscale_obs(state, to_gray=True)).float()\n",
    "    tmp = state_.repeat((N,1,1))\n",
    "    return tmp.unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 8.4 - the policy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(qvalues, eps=None): #1: input for the policy function: action values, epsilon parameter\n",
    "    if eps is not None:\n",
    "        if torch.rand(1) < eps:\n",
    "            return torch.randint(low=0,high=7,size=(1,))\n",
    "        else:\n",
    "            return torch.argmax(qvalues)\n",
    "    else:\n",
    "        return torch.multinomial(F.softmax(F.normalize(qvalues)), num_samples=1) #2: if eps not provided -> sample from the softmax (multinomial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 8.5 - experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ExperienceReplay:\n",
    "    def __init__(self, N=500, batch_size=100):\n",
    "        self.N = N #1: the maximum size of the memory list\n",
    "        self.batch_size = batch_size #2: the number of samples to generate from the memory (/w get_batch method)\n",
    "        self.memory = [] \n",
    "        self.counter = 0\n",
    "        \n",
    "    def add_memory(self, state1, action, reward, state2):\n",
    "        self.counter +=1 \n",
    "        if self.counter % 500 == 0: #3: every 500 iterations -> shuffles the memory list -> promote a more random sample\n",
    "            self.shuffle_memory()\n",
    "            \n",
    "        if len(self.memory) < self.N: #4: if the memory is not full -> adds to the list; else -> replaces a random memory with the new one\n",
    "            self.memory.append( (state1, action, reward, state2) )\n",
    "        else:\n",
    "            rand_index = np.random.randint(0,self.N-1)\n",
    "            self.memory[rand_index] = (state1, action, reward, state2)\n",
    "    \n",
    "    def shuffle_memory(self): #5: to shuffle the memory list\n",
    "        shuffle(self.memory)\n",
    "        \n",
    "    def get_batch(self): #6: randomly samples a mini-batch from the memory list\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            batch_size = len(self.memory)\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "        if len(self.memory) < 1:\n",
    "            print(\"Error: No data in memory.\")\n",
    "            return None\n",
    "        #7: creates an array of random integers representing indices\n",
    "        ind = np.random.choice(np.arange(len(self.memory)),batch_size,replace=False)\n",
    "        batch = [self.memory[i] for i in ind] #batch is a list of tuples\n",
    "        state1_batch = torch.stack([x[0].squeeze(dim=0) for x in batch],dim=0)\n",
    "        action_batch = torch.Tensor([x[1] for x in batch]).long()\n",
    "        reward_batch = torch.Tensor([x[2] for x in batch])\n",
    "        state2_batch = torch.stack([x[3].squeeze(dim=0) for x in batch],dim=0)\n",
    "        return state1_batch, action_batch, reward_batch, state2_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 8.6 - intrinsic curiosity module (ICM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phi(nn.Module): #1: encoder network (ø) - four convolutional layers\n",
    "    def __init__(self):\n",
    "        super(Phi, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=2, padding=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.normalize(x)\n",
    "        y = F.elu(self.conv1(x))\n",
    "        y = F.elu(self.conv2(y))\n",
    "        y = F.elu(self.conv3(y))\n",
    "        y = F.elu(self.conv4(y)) #size [1, 32, 3, 3] batch, channels, 3 x 3\n",
    "        y = y.flatten(start_dim=1) #size N, 288\n",
    "        return y\n",
    "\n",
    "class Gnet(nn.Module): #2: inverse model - a simple two-layer neural network with linear layers\n",
    "    def __init__(self):\n",
    "        super(Gnet, self).__init__()\n",
    "        self.linear1 = nn.Linear(576,256)\n",
    "        self.linear2 = nn.Linear(256,12)\n",
    "\n",
    "    def forward(self, state1,state2):\n",
    "        x = torch.cat( (state1, state2) ,dim=1)\n",
    "        y = F.relu(self.linear1(x))\n",
    "        y = self.linear2(y)\n",
    "        y = F.softmax(y,dim=1)\n",
    "        return y\n",
    "\n",
    "class Fnet(nn.Module): #3: forward model - a simple two-layer neural network with linear layers\n",
    "    def __init__(self):\n",
    "        super(Fnet, self).__init__()\n",
    "        self.linear1 = nn.Linear(300,256)\n",
    "        self.linear2 = nn.Linear(256,288)\n",
    "\n",
    "    def forward(self,state,action):\n",
    "        action_ = torch.zeros(action.shape[0],12) #4: actions are stores as integers in the replay memory -> ocnvert to a one-hot encoded vector\n",
    "        indices = torch.stack( (torch.arange(action.shape[0]), action.squeeze()), dim=0)\n",
    "        indices = indices.tolist()\n",
    "        action_[indices] = 1.\n",
    "        x = torch.cat( (state,action_) ,dim=1)\n",
    "        y = F.relu(self.linear1(x))\n",
    "        y = self.linear2(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 8.7 - Deep Q-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qnetwork, self).__init__()\n",
    "        #in_channels, out_channels, kernel_size, stride=1, padding=0\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.linear1 = nn.Linear(288,100)\n",
    "        self.linear2 = nn.Linear(100,12)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.normalize(x)\n",
    "        y = F.elu(self.conv1(x))\n",
    "        y = F.elu(self.conv2(y))\n",
    "        y = F.elu(self.conv3(y))\n",
    "        y = F.elu(self.conv4(y))\n",
    "        y = y.flatten(start_dim=2)\n",
    "        y = y.view(y.shape[0], -1, 32)\n",
    "        y = y.flatten(start_dim=1)\n",
    "        y = F.elu(self.linear1(y))\n",
    "        y = self.linear2(y) # output size: (N, 12)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 8.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size':150,\n",
    "    'beta':0.2,\n",
    "    'lambda':0.1,\n",
    "    'eta': 1.0,\n",
    "    'gamma':0.2,\n",
    "    'max_episode_len':100,\n",
    "    'min_progress':15,\n",
    "    'action_repeats':6,\n",
    "    'frames_per_state':3\n",
    "}\n",
    "\n",
    "replay = ExperienceReplay(N=1000, batch_size=params['batch_size'])\n",
    "Qmodel = Qnetwork()\n",
    "encoder = Phi()\n",
    "forward_model = Fnet()\n",
    "inverse_model = Gnet()\n",
    "forward_loss = nn.MSELoss(reduction='none')\n",
    "inverse_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "qloss = nn.MSELoss()\n",
    "all_model_params = list(Qmodel.parameters()) + list(encoder.parameters()) #A\n",
    "all_model_params += list(forward_model.parameters()) + list(inverse_model.parameters())\n",
    "opt = optim.Adam(lr=0.001, params=all_model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 8.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(q_loss, inverse_loss, forward_loss):\n",
    "    loss_ = (1 - params['beta']) * inverse_loss\n",
    "    loss_ += params['beta'] * forward_loss\n",
    "    loss_ = loss_.sum() / loss_.flatten().shape[0]\n",
    "    loss = loss_ + params['lambda'] * q_loss\n",
    "    return loss\n",
    "\n",
    "def reset_env():\n",
    "    \"\"\"\n",
    "    Reset the environment and return a new initial state\n",
    "    \"\"\"\n",
    "    env.reset()\n",
    "    state1 = prepare_initial_state(env.render('rgb_array'))\n",
    "    return state1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 8.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICM(state1, action, state2, forward_scale=1., inverse_scale=1e4):\n",
    "    state1_hat = encoder(state1) #A\n",
    "    state2_hat = encoder(state2)\n",
    "    state2_hat_pred = forward_model(state1_hat.detach(), action.detach()) #B\n",
    "    forward_pred_err = forward_scale * forward_loss(state2_hat_pred, \\\n",
    "                        state2_hat.detach()).sum(dim=1).unsqueeze(dim=1)\n",
    "    pred_action = inverse_model(state1_hat, state2_hat) #C\n",
    "    inverse_pred_err = inverse_scale * inverse_loss(pred_action, \\\n",
    "                                        action.detach().flatten()).unsqueeze(dim=1)\n",
    "    return forward_pred_err, inverse_pred_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 8.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_train(use_extrinsic=True):\n",
    "    state1_batch, action_batch, reward_batch, state2_batch = replay.get_batch() \n",
    "    action_batch = action_batch.view(action_batch.shape[0],1) #A\n",
    "    reward_batch = reward_batch.view(reward_batch.shape[0],1)\n",
    "    \n",
    "    forward_pred_err, inverse_pred_err = ICM(state1_batch, action_batch, state2_batch) #B\n",
    "    i_reward = (1. / params['eta']) * forward_pred_err #C\n",
    "    reward = i_reward.detach() #D\n",
    "    if use_explicit: #E\n",
    "        reward += reward_batch \n",
    "    qvals = Qmodel(state2_batch) #F\n",
    "    reward += params['gamma'] * torch.max(qvals)\n",
    "    reward_pred = Qmodel(state1_batch)\n",
    "    reward_target = reward_pred.clone()\n",
    "    indices = torch.stack( (torch.arange(action_batch.shape[0]), \\\n",
    "    action_batch.squeeze()), dim=0)\n",
    "    indices = indices.tolist()\n",
    "    reward_target[indices] = reward.squeeze()\n",
    "    q_loss = 1e5 * qloss(F.normalize(reward_pred), F.normalize(reward_target.detach()))\n",
    "    return forward_pred_err, inverse_pred_err, q_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 8.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/unam9su/.local/share/virtualenvs/DeepReinforcementLearningInAction-fvkxvmmz/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "epochs = 5000\n",
    "env.reset()\n",
    "state1 = prepare_initial_state(env.render('rgb_array'))\n",
    "eps=0.15\n",
    "losses = []\n",
    "episode_length = 0\n",
    "switch_to_eps_greedy = 1000\n",
    "state_deque = deque(maxlen=params['frames_per_state'])\n",
    "e_reward = 0.\n",
    "last_x_pos = env.env.env._x_position #A\n",
    "ep_lengths = []\n",
    "use_explicit = False\n",
    "for i in range(epochs):\n",
    "    opt.zero_grad()\n",
    "    episode_length += 1\n",
    "    q_val_pred = Qmodel(state1) #B\n",
    "    if i > switch_to_eps_greedy: #C\n",
    "        action = int(policy(q_val_pred,eps))\n",
    "    else:\n",
    "        action = int(policy(q_val_pred))\n",
    "    for j in range(params['action_repeats']): #D\n",
    "        state2, e_reward_, done, info = env.step(action)\n",
    "        last_x_pos = info['x_pos']\n",
    "        if done:\n",
    "            state1 = reset_env()\n",
    "            break\n",
    "        e_reward += e_reward_\n",
    "        state_deque.append(prepare_state(state2))\n",
    "    state2 = torch.stack(list(state_deque),dim=1) #E\n",
    "    replay.add_memory(state1, action, e_reward, state2) #F\n",
    "    e_reward = 0\n",
    "    if episode_length > params['max_episode_len']: #G\n",
    "        if (info['x_pos'] - last_x_pos) < params['min_progress']:\n",
    "            done = True\n",
    "        else:\n",
    "            last_x_pos = info['x_pos']\n",
    "    if done:\n",
    "        ep_lengths.append(info['x_pos'])\n",
    "        state1 = reset_env()\n",
    "        last_x_pos = env.env.env._x_position\n",
    "        episode_length = 0\n",
    "    else:\n",
    "        state1 = state2\n",
    "    if len(replay.memory) < params['batch_size']:\n",
    "        continue\n",
    "    forward_pred_err, inverse_pred_err, q_loss = minibatch_train(use_extrinsic=False) #H\n",
    "    loss = loss_fn(q_loss, forward_pred_err, inverse_pred_err) #I\n",
    "    loss_list = (q_loss.mean(), forward_pred_err.flatten().mean(),\\\n",
    "    inverse_pred_err.flatten().mean())\n",
    "    losses.append(loss_list)\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = True\n",
    "state_deque = deque(maxlen=params['frames_per_state'])\n",
    "for step in range(5000):\n",
    "    if done:\n",
    "        env.reset()\n",
    "        state1 = prepare_initial_state(env.render('rgb_array'))\n",
    "    q_val_pred = Qmodel(state1)\n",
    "    action = int(policy(q_val_pred,eps))\n",
    "    state2, reward, done, info = env.step(action)\n",
    "    state2 = prepare_multi_state(state1,state2)\n",
    "    state1=state2\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
